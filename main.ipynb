{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T11:58:16.179447Z",
     "start_time": "2025-11-30T11:58:16.156839Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 1. GROQ CONFIG\n",
    "# =============================\n",
    "GROQ_API_KEY = \"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\"\n",
    "#GROQ_URL = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "GROQ_MODEL = \"llama-3.1-8b-instant\"\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 2. GROQ LLM WRAPPER\n",
    "# =============================\n",
    "class GroqLLM(LLM):\n",
    "    api_key: str = GROQ_API_KEY\n",
    "    model: str = GROQ_MODEL\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"groq_custom\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.2\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        res = requests.post(GROQ_URL, json=payload, headers=headers)\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            raise HTTPException(status_code=500, detail=f\"Groq API Error: {res.text}\")\n",
    "\n",
    "        return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "llm = GroqLLM()\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 3. FAST TF-IDF RETRIEVER\n",
    "# =============================\n",
    "class FastRetriever:\n",
    "    def __init__(self, docs: List[str]):\n",
    "        self.docs = docs\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.matrix = self.vectorizer.fit_transform(docs)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 3):\n",
    "        q_vec = self.vectorizer.transform([query])\n",
    "        scores = (q_vec @ self.matrix.T).toarray()[0]\n",
    "\n",
    "        idx = np.argsort(scores)[::-1][:top_k]\n",
    "        return [self.docs[i] for i in idx if scores[i] > 0.03]   # threshold\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 4. LOAD FAQ\n",
    "# =============================\n",
    "with open(\"faq.json\", \"r\") as f:\n",
    "    FAQ: Dict[str, str] = json.load(f)\n",
    "\n",
    "documents = list(FAQ.values())\n",
    "retriever = FastRetriever(documents)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 5. MEMORY\n",
    "# =============================\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\"\n",
    ")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 6. PROMPT TEMPLATE\n",
    "# =============================\n",
    "template = \"\"\"\n",
    "You are a polite customer service assistant.\n",
    "\n",
    "Use:\n",
    "- Retrieved context\n",
    "- Full conversation history\n",
    "\n",
    "If unsure or missing context, respond exactly:\n",
    "\"I'm not fully sure about this. Please contact the service provider for more assistance.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "User Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 7. FASTAPI SETUP\n",
    "# =============================\n",
    "app = FastAPI(title=\"Groq Customer Service Bot + TTS\")\n",
    "\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    user_id: str = \"default\"\n",
    "    message: str\n",
    "\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 8. /chat ENDPOINT\n",
    "# =============================\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "def chat(req: ChatRequest):\n",
    "    question = req.message\n",
    "\n",
    "    # Retrieve context\n",
    "    ctx_docs = retriever.search(question)\n",
    "    context = \"\\n\".join(ctx_docs) if ctx_docs else \"No relevant FAQ found.\"\n",
    "\n",
    "    # LangChain pipeline\n",
    "    answer = chain.run(context=context, question=question)\n",
    "\n",
    "    # fallback\n",
    "    if any(x in answer.lower() for x in [\"not sure\", \"don't know\", \"contact\"]):\n",
    "        answer = \"I'm not fully sure about this. Please contact the service provider for more assistance.\"\n",
    "\n",
    "    return ChatResponse(answer=answer.strip())\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 9. TTS (ElevenLabs - Bella voice)\n",
    "# =============================\n",
    "ELEVEN_API_KEY = \"\"\n",
    "#ELEVEN_VOICE_ID = \"EXAVITQu4vr4xnSDxMaL\"  # Bella\n",
    "#ELEVEN_URL = f\"https://api.elevenlabs.io/v1/text-to-speech/{ELEVEN_VOICE_ID}/stream\"\n",
    "\n",
    "\n",
    "@app.post(\"/tts\")\n",
    "def text_to_speech(text: str):\n",
    "    headers = {\n",
    "        \"Accept\": \"audio/mpeg\",\n",
    "        \"xi-api-key\": ELEVEN_API_KEY,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": 0.4,\n",
    "            \"similarity_boost\": 0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "    res = requests.post(ELEVEN_URL, json=payload, headers=headers, stream=True)\n",
    "\n",
    "    if res.status_code != 200:\n",
    "        raise HTTPException(status_code=500, detail=res.text)\n",
    "\n",
    "    return StreamingResponse(\n",
    "        res.iter_content(chunk_size=1024),\n",
    "        media_type=\"audio/mpeg\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Run with:\n",
    "# uvicorn main:app --reload --port 8000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306ed11324db005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
